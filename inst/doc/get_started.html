<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Motivation, Installation and Quick Workflow</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 800px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
font-size: 16px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 0px;
padding: 15px;
width: 770px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 16px;
line-height: 1.5;
}
#TOC .toctitle {
font-weight: bold;
font-size: 16px;
margin-left: 5px;
}
#TOC ul {
padding-left: 50px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 20px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
text-align: justify;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 5px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 5px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: "Lucida Console", Monaco, Consolas, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 25px;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 1px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
margin-left: 8px;
font-size: 105%;
}
em {
font-style: normal;
}
emph {
font-style: oblique;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #123d79;
text-decoration: none;
}
a:hover {
color: #007CC3; }
a:visited {
color: #581858; }
a:visited:hover {
color: #007CC3; }

code > span.kw { color: #4271ae; } 
code > span.dt { color: #c82829; } 
code > span.dv { color: #f5871f; } 
code > span.bn { color: #718c00; } 
code > span.fl { color: #718c00; } 
code > span.ch { color: #718c00; } 
code > span.st { color: #718c00; } 
code > span.co { color: #8e908c; } 
code > span.ot { color: #4d4d4c; } 
code > span.al { color: #ff0000; } 
code > span.fu { color: #4271ae; } 
code > span.er { color: #a61717; } 

img {
display: block;
margin: 0 auto;
}
</style>




</head>

<body>




<h1 class="title toc-ignore">Motivation, Installation and Quick
Workflow</h1>
<h3 class="subtitle"><small> <a href="http://r-addict.com/About.html">Marcin Kosinski</a>
  <a href="https://stackoverflow.com/users/3857701"><i class="fa fa-stack-overflow"></i></a>  <a href="http://r-addict.com"><i class="fa fa-comment"></i></a>  <a href="https://github.com/MarcinKosinski"><i class="fa fa-github"></i></a>  <a href="mailto:m.p.kosinski@gmail.com"><i class="fa fa-envelope-o"></i></a>
<br> <a href="https://www.zstat.pl/">Zygmunt Zawadzki</a>
  <a href="https://www.zstat.pl/"><i class="fa fa-comment"></i></a>  <a href="https://github.com/zzawadz"><i class="fa fa-github"></i></a>  <a href="mailto:zygmunt@zstat.pl"><i class="fa fa-envelope-o"></i></a></small><br></h3>


<div id="TOC">
<ul>
<li><a href="#installation" id="toc-installation">Installation</a></li>
<li><a href="#motivation" id="toc-motivation">Motivation</a></li>
<li><a href="#quick-workflow" id="toc-quick-workflow">Quick
Workflow</a></li>
<li><a href="#use-cases" id="toc-use-cases">Use cases</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<p><a href="https://mi2-warsaw.github.io/FSelectorRcpp/">FSelectorRcpp</a> is
an <a href="https://CRAN.R-project.org/package=Rcpp">Rcpp</a> (free of
Java/Weka) implementation of <a href="https://CRAN.R-project.org/package=FSelector">FSelector</a>
entropy-based feature selection algorithms with a sparse matrix support.
It is also equipped with a parallel backend.</p>
<div id="installation" class="section level2">
<h2>Installation</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&#39;FSelectorRcpp&#39;</span>) <span class="co"># stable release version on CRAN</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&#39;mi2-warsaw/FSelectorRcpp&#39;</span>) <span class="co"># dev version</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># windows users should have Rtools for devtools installation</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># https://cran.r-project.org/bin/windows/Rtools/</span></span></code></pre></div>
</div>
<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>In the modern statistical learning the biggest bottlenecks are
computation times of model training procedures and the overfitting. Both
are caused by the same issue - the high dimension of explanatory
variables space. Researchers have encountered problems with too big sets
of features used in machine learning algorithms also in terms of model
interpretaion. This motivates applying feature selection algorithms
before performing statisical modeling, so that on smaller set of
attributes the training time will be shorter, the interpretation might
be clearer and the noise from non important features can be avoided.
More motivation can be found in <span class="citation">(John, Kohavi,
and Pfleger 1994)</span>.</p>
<p>Many methods were developed to reduce the curse of dimensionality
like <strong>Principal Component Analysis</strong> <span class="citation">(Pearson 1901)</span> or <strong>Singular Value
Decomposition</strong> <span class="citation">(Eckart and Young
1936)</span> which approximates the variables by smaller number of
combinations of original variables, but this approach is hard to
interpret in the final model.</p>
<p>Sophisticated methods of attribute selection as
<strong>Boruta</strong> algoritm <span class="citation">(Kursa and
Rudnicki 2010)</span>, genetic algorithms <span class="citation">Aziz et
al. (2013)</span> or simulated annealing techniques <span class="citation">(Khachaturyan, Semenovsovskaya, and Vainshtein
1981)</span> are known and broadly used but in some cases for those
algorithms computations can take even days, not to mention that datasets
are growing every hour.</p>
<p>Few classification and regression models can reduce redundand
variables during the training phase of statistical learning process,
e.g. <strong>Decision Trees</strong> <span class="citation">Breiman et
al. (1984)</span>, <strong>LASSO Regularized Generalized Linear
Models</strong> (with cross-validation) <span class="citation">(Friedman, Hastie, and Tibshirani 2010)</span> or
<strong>Regularized Support Vector Machine</strong> <span class="citation">(Xu, Caramanis, and Mannor 2009)</span>, but still
computations starting with full set of explanatory variables are time
consuming and the understaning of the feature selection procedure in
this case is not simple and those methods are sometimes used without the
understanding.</p>
<p>In business applications there appear a need to provide a fast
feature selection that is extremely easy to understand. For such demands
easy methods are prefered. This motivates using simple techniques like
<strong>Entropy Based Feature Selection</strong> <span class="citation">(Largeron, Moulin, and Géry 2011)</span>, where every
feature can be checked independently so that computations can be
performed in a parallel to shorter the procedure’s time. For this
approach we provide an R interface to <strong>Rcpp</strong>
reimplementation <span class="citation">(Dirk Eddelbuettel 2011)</span>
of methods included in <strong>FSelector</strong> package which we also
extended with parallel background and sparse matrix support. This has
significant impact on computations time and can be used on greater
datasets, comparing to <strong>FSelector</strong>. Additionally we
avoided the Weka <span class="citation">(Hall et al. 2009)</span>
dependency and we provided faster discretization implementations than
those from <strong>entropy</strong> package, used originally in
<strong>FSelector</strong>.</p>
</div>
<div id="quick-workflow" class="section level2">
<h2>Quick Workflow</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(magrittr)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(FSelectorRcpp)</span></code></pre></div>
<p>A simple entropy based feature selection workflow.
<strong>Information gain</strong> is an easy, linear algorithm that
computes the entropy of a dependent and explanatory variables, and the
conditional entropy of a dependent variable with a respect to each
explanatory variable separately. This simple statistic enables to
calculate the belief of the distribution of a dependent variable when we
only know the distribution of a explanatory variable.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">information_gain</span>(               <span class="co"># Calculate the score for each attribute</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">formula =</span> Species <span class="sc">~</span> .,      <span class="co"># that is on the right side of the formula.</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> iris,                <span class="co"># Attributes must exist in the passed data.</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">type  =</span> <span class="st">&quot;infogain&quot;</span>          <span class="co"># Choose the type of a score to be calculated.</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span>                          </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cut_attrs</span>(                    <span class="co"># Then take attributes with the highest rank.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">k =</span> <span class="dv">2</span>                       <span class="co"># For example: 2 attrs with the higehst rank.</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span>                         </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">to_formula</span>(                   <span class="co"># Create a new formula object with </span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">attrs =</span> .,                  <span class="co"># the most influencial attrs.</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">class =</span> <span class="st">&quot;Species&quot;</span>           </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">glm</span>(</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">formula =</span> .,                <span class="co"># Use that formula in any classification algorithm.</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> iris,                </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>         </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>
Call:  glm(formula = ., family = &quot;binomial&quot;, data = iris)

Coefficients:
 (Intercept)   Petal.Width  Petal.Length  
      -69.45         33.89         17.60  

Degrees of Freedom: 149 Total (i.e. Null);  147 Residual
Null Deviance:      191 
Residual Deviance: 5.17e-09     AIC: 6</code></pre>
<p>Apply a complicated feature selection search engine, that checks
combinations of subsets of the specified attributes’ set, to score the
currently considered subset, depending on the criterion passed with the
<code>fun</code> parameter.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>evaluator_R2_lm <span class="ot">&lt;-</span>    <span class="co"># Create a scorer function.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    attributes,       <span class="co"># That takes the currently considered subset of attributes</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    data,             <span class="co"># from a specified dataset.</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">dependent =</span> </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">names</span>(data)[<span class="dv">1</span>]  <span class="co"># To find features that best describe the dependent variable.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  ) {</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summary</span>(          <span class="co"># In this situation we take the r.squared statistic</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">lm</span>(             <span class="co"># from the summary of a linear model object.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">to_formula</span>(   <span class="co"># This is the score to use to choose between considered </span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>          attributes, <span class="co"># subsets of attributes.</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>          dependent</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">data =</span> data)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    )<span class="sc">$</span>r.squared</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="fu">feature_search</span>(          <span class="co"># feature_search work in 2 modes - &#39;exhaustive&#39; and &#39;greedy&#39;</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">attributes =</span> </span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">names</span>(iris)[<span class="sc">-</span><span class="dv">1</span>],     <span class="co"># It takes attribues and creates combinations of it&#39;s subsets.</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">fun =</span> evaluator_R2_lm, <span class="co"># And it calculates the score of a subset that depends on the </span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> iris,           <span class="co"># evaluator function passed in the `fun` parameter.</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">mode =</span> <span class="st">&quot;exhaustive&quot;</span>,   <span class="co"># exhaustive - means to check all possible </span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">sizes =</span>                <span class="co"># attributes&#39; subset combinations </span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(attributes) <span class="co"># of sizes passed in sizes.</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>)<span class="sc">$</span>all</span></code></pre></div>
<pre><code>  Sepal.Width Petal.Length Petal.Width Species     values
1           1            0           0       0 0.01382265
2           0            1           0       0  0.7599546
3           0            0           1       0  0.6690277
4           0            0           0       1  0.6187057</code></pre>
</div>
<div id="use-cases" class="section level2">
<h2>Use cases</h2>
<ul>
<li><a href="http://r-addict.com/2017/01/08/Entropy-Based-Image-Binarization.html">Entropy
Based Image Binarization with imager and FSelectorRcpp</a></li>
<li><a href="https://www.r-bloggers.com/2016/06/venn-diagram-comparison-of-boruta-fselectorrcpp-and-glmnet-algorithms/">Venn
Diagram Comparison of Boruta, FSelectorRcpp and GLMnet
Algorithms</a></li>
</ul>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-FedCSIS2013l106" class="csl-entry">
Aziz, Amira Sayed A., Ahmad Taher Azar, Mostafa A. Salama, Aboul Ella
Hassanien, and Sanaa El Ola Hanfy. 2013. <span>“Genetic Algorithms with
Different Feature Selection Techniques for Anomaly Detectors
Generation.”</span> In <em>Proceedings of the 2013 Federated Conference
on Computer Science and Information Systems</em>, edited by M. Paprzycki
M. Ganzha L. Maciaszek, pages 769–774. IEEE.
</div>
<div id="ref-cart84" class="csl-entry">
Breiman, L., J. Friedman, R. Olshen, and C. Stone. 1984. <em><span class="nocase">Classification and Regression Trees</span></em>.
Monterey, CA: Wadsworth; Brooks.
</div>
<div id="ref-Rcpp" class="csl-entry">
Dirk Eddelbuettel, Romain Franccois. 2011. <span>“<span>Rcpp</span>:
Seamless <span>R</span> and <span>C++</span> Integration.”</span>
<em>Journal of Statistical Software</em> 40 (8): 1–18. <a href="https://www.jstatsoft.org/v40/i08/">https://www.jstatsoft.org/v40/i08/</a>.
</div>
<div id="ref-eckart1936approximation" class="csl-entry">
Eckart, C., and G. Young. 1936. <span>“The Approximation of One Matrix
by Another of Lower Rank.”</span> <em>Psychometrika</em> 1 (3): 211–18.
<a href="https://doi.org/10.1007/BF02288367">https://doi.org/10.1007/BF02288367</a>.
</div>
<div id="ref-glmnet" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010.
<span>“Regularization Paths for Generalized Linear Models via Coordinate
Descent.”</span> <em>Journal of Statistical Software</em> 33 (1): 1–22.
<a href="https://www.jstatsoft.org/v33/i01/">https://www.jstatsoft.org/v33/i01/</a>.
</div>
<div id="ref-Hall:2009:WDM:1656274.1656278" class="csl-entry">
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter
Reutemann, and Ian H. Witten. 2009. <span>“The WEKA Data Mining
Software: An Update.”</span> <em>SIGKDD Explor. Newsl.</em> 11 (1):
10–18. <a href="https://doi.org/10.1145/1656274.1656278">https://doi.org/10.1145/1656274.1656278</a>.
</div>
<div id="ref-John94irrelevantfeatures" class="csl-entry">
John, George H., Ron Kohavi, and Karl Pfleger. 1994. <span>“Irrelevant
Features and the Subset Selection Problem.”</span> In <em>MACHINE
LEARNING: PROCEEDINGS OF THE ELEVENTH INTERNATIONAL</em>, 121–29. Morgan
Kaufmann.
</div>
<div id="ref-Khachaturyan:a19748" class="csl-entry">
Khachaturyan, A., S. Semenovsovskaya, and B. Vainshtein. 1981.
<span>“<span class="nocase">The thermodynamic approach to the structure
analysis of crystals</span>.”</span> <em>Acta Crystallographica Section
A</em> 37 (5): 742–54.
</div>
<div id="ref-geneticAlgo" class="csl-entry">
Kuhn, Max, and Kjell Johnson. 2013. <span>“Applied Predictive
Modeling.”</span> New York, NY: Springer. 2013. <a href="https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/">https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/</a>.
</div>
<div id="ref-Boruta" class="csl-entry">
Kursa, Miron B., and Witold R. Rudnicki. 2010. <span>“Feature Selection
with the <span>Boruta</span> Package.”</span> <em>Journal of Statistical
Software</em> 36 (11): 1–13. <a href="https://www.jstatsoft.org/v36/i11/">https://www.jstatsoft.org/v36/i11/</a>.
</div>
<div id="ref-Largeron:2011:EBF:1982185.1982389" class="csl-entry">
Largeron, Christine, Christophe Moulin, and Mathias Géry. 2011.
<span>“Entropy Based Feature Selection for Text Categorization.”</span>
In <em>Proceedings of the 2011 ACM Symposium on Applied Computing</em>,
924–28. SAC ’11. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/1982185.1982389">https://doi.org/10.1145/1982185.1982389</a>.
</div>
<div id="ref-PCA:14786440109462720" class="csl-entry">
Pearson, Karl. 1901. <span>“LIII. On Lines and Planes of Closest Fit to
Systems of Points in Space.”</span> <em>Philosophical Magazine Series
6</em> 2 (11): 559–72.
</div>
<div id="ref-Rokach:2008:DMD:1796114" class="csl-entry">
Rokach, Lior, and Oded Maimon. 2008. <em>Data Mining with Decision
Trees: Theroy and Applications</em>. River Edge, NJ, USA: World
Scientific Publishing Co., Inc.
</div>
<div id="ref-Xu:2009:RRS:1577069.1755834" class="csl-entry">
Xu, Huan, Constantine Caramanis, and Shie Mannor. 2009.
<span>“Robustness and Regularization of Support Vector Machines.”</span>
<em>J. Mach. Learn. Res.</em> 10 (December): 1485–1510. <a href="https://dl.acm.org/doi/10.5555/1577069.1755834">https://dl.acm.org/doi/10.5555/1577069.1755834</a>.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
